{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:31:02.848486Z",
     "start_time": "2017-12-22T02:31:02.792617Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk.sentiment.util as sentimentutils\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:25:38.664991Z",
     "start_time": "2017-12-22T02:25:38.657139Z"
    }
   },
   "outputs": [],
   "source": [
    "read_file = lambda x : [tuple(line.strip().split(\"\\t\")) for line in open(x).readlines() if len(line.strip()) > 0]\n",
    "assign_class = lambda x, y: [l + (y, ) for l in x]\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:25:41.336259Z",
     "start_time": "2017-12-22T02:25:41.317674Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_class, neg_class = 'P', 'N'\n",
    "pos_lines = assign_class(read_file(\"hotelPosT-train.txt\"), pos_class)\n",
    "neg_lines = assign_class(read_file(\"hotelNegT-train.txt\"), neg_class)\n",
    "\n",
    "sentiment_lines = pos_lines + neg_lines\n",
    "raw_sentiment_pd = pd.DataFrame(sentiment_lines, columns = [\"id\", \"sentence\", \"class\"])\n",
    "raw_sentiment_pd = shuffle(raw_sentiment_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:25:42.266397Z",
     "start_time": "2017-12-22T02:25:42.243920Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentiments):\n",
    "        features = []\n",
    "        stemmer = PorterStemmer()\n",
    "        for sentiment in sentiments:\n",
    "\n",
    "            feature = []\n",
    "            sentences = nltk.sent_tokenize(sentiment)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                tokens = [\n",
    "                    stemmer.stem(token)\n",
    "                    for token in nltk.word_tokenize(sentence)\n",
    "                ]\n",
    "                feature.extend([\n",
    "                    token.lower()\n",
    "                    for token in sentimentutils.mark_negation(\n",
    "                        tokens, double_neg_flip=True) if token.isalpha()\n",
    "                ])\n",
    "\n",
    "            features.append(\" \".join(feature))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:25:48.236562Z",
     "start_time": "2017-12-22T02:25:44.123738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.936699857752\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = Pipeline([\n",
    "    ('tokenizer', SentimentTokenizer()),\n",
    "    ('bag_of_words', TfidfVectorizer(min_df = 0.0, max_df = 1.0)),\n",
    "    (\"clf\", BernoulliNB())\n",
    "])\n",
    "\n",
    "sentiments = raw_sentiment_pd[\"sentence\"].tolist()\n",
    "sentiment_labels = raw_sentiment_pd[\"class\"].tolist()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 5)\n",
    "\n",
    "accuracy = []\n",
    "for train_idx, test_idx in kf.split(sentiments, sentiment_labels):\n",
    "    X_train, X_test = [sentiments[i]\n",
    "                       for i in train_idx], [sentiments[i] for i in test_idx]\n",
    "    y_train, y_test = [sentiment_labels[i]\n",
    "                       for i in train_idx], [sentiment_labels[i] for i in test_idx]\n",
    "    sentiment_pipeline.fit(X_train, y_train)\n",
    "    prediction = sentiment_pipeline.predict(X_test)\n",
    "    accuracy.append(np.sum(y_test == prediction) * 1.0 / len(y_test))\n",
    "print np.mean(accuracy)\n",
    "\n",
    "sentiment_classifier = sentiment_pipeline.fit(sentiments, sentiment_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data for Deception Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T02:25:54.683808Z",
     "start_time": "2017-12-22T02:25:54.657114Z"
    }
   },
   "outputs": [],
   "source": [
    "true_class, false_class = 'T', 'F'\n",
    "\n",
    "#  Reading the data and converting into DataFrame\n",
    "\n",
    "true_lines = assign_class(read_file(\"./hotelT-train.txt\"), true_class)\n",
    "false_lines = assign_class(read_file(\"./hotelF-train.txt\"), false_class)\n",
    "\n",
    "all_lines = true_lines + false_lines\n",
    "raw_data_pd = pd.DataFrame(all_lines, columns=[\"id\", \"sentence\", \"class\"])\n",
    "raw_data_pd = shuffle(raw_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Deception Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T03:58:11.143517Z",
     "start_time": "2017-12-22T03:58:11.093227Z"
    }
   },
   "outputs": [],
   "source": [
    "class OpinionExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, opinions):\n",
    "        features = np.recarray(\n",
    "            shape=(len(opinions), ),\n",
    "            dtype=[('opinion', object), ('tokens_per_sent', object), ('pos_per_sent', object),\n",
    "                   ('tokens', object), ('pos', object), ('sent_seg_opinion', object),\n",
    "                   ('sent_seg_opinion_token', object), ('sent_seg_opinion_token_pos', object)])\n",
    "        for i, opinion in enumerate(opinions):\n",
    "            features['opinion'][i] = opinion\n",
    "            features['sent_seg_opinion'][i] = nltk.sent_tokenize(opinion)\n",
    "            features['sent_seg_opinion_token'][i] = [\n",
    "                map(str.lower, nltk.word_tokenize(sent))\n",
    "                for sent in features['sent_seg_opinion'][i]\n",
    "            ]\n",
    "            features['sent_seg_opinion_token_pos'][i] = nltk.pos_tag_sents(\n",
    "                features['sent_seg_opinion_token'][i])\n",
    "            features['tokens_per_sent'][i], features['pos_per_sent'][i] = [], []\n",
    "\n",
    "            for sent in features['sent_seg_opinion_token_pos'][i]:\n",
    "                features['tokens_per_sent'][i].append(\" \".join(\n",
    "                    [token for token, pos in sent if token.isalpha()]))\n",
    "                features['pos_per_sent'][i].append(\" \".join(\n",
    "                    [pos for token, pos in sent]))\n",
    "\n",
    "            features['tokens'][i] = \" \".join(features['tokens_per_sent'][i])\n",
    "            features['pos'][i] = \" \".join(features['pos_per_sent'][i])\n",
    "        return features\n",
    "\n",
    "\n",
    "class OpinionStats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, opinions_sents):\n",
    "        stats = [{\n",
    "            'length': sum(map(len, opinion_sents)),\n",
    "            'num_sentences': len(opinion_sents)\n",
    "        } for opinion_sents in opinions_sents]\n",
    "        return stats\n",
    "\n",
    "\n",
    "class SentimentExtractor(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def fit(self, sentiments,labels):\n",
    "        self.classifier = sentiment_pipeline.fit(sentiments, labels)\n",
    "        return self\n",
    "\n",
    "    def transform(self, opinions):\n",
    "        return [{\"sentiment\": sentiment} for sentiment in self.classifier.predict(opinions)]\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "memory = Memory(cachedir=cachedir, verbose = 0)\n",
    "pipeline = Pipeline([\n",
    "    ('opinions', OpinionExtractor()),\n",
    "    ('union',\n",
    "     FeatureUnion(\n",
    "         transformer_list=[\n",
    "#              ('word_tokens',\n",
    "#               Pipeline([('selector', ItemSelector(key='tokens')),\n",
    "#                         ('tfidf', TfidfVectorizer(stop_words=\"english\")),\n",
    "#                         ('best', SelectKBest(chi2))])),\n",
    "             ('pos_tokens',\n",
    "              Pipeline([\n",
    "                  ('selector', ItemSelector(key='pos')), \n",
    "                  ('tfidf', TfidfVectorizer())\n",
    "              ])),\n",
    "#              ('opinion_stats',\n",
    "#               Pipeline([('selector',ItemSelector(key='sent_seg_opinion')),\n",
    "#                         ('stat', OpinionStats()), ('vect', DictVectorizer())])),\n",
    "             ('sentiments',\n",
    "              Pipeline([('selection', ItemSelector(key='opinion')),\n",
    "                  ('sentiment', SentimentExtractor()), ('vect', DictVectorizer())\n",
    "              ]))\n",
    "         ],\n",
    "         transformer_weights={\n",
    "#              'word_tokens': 0.0,\n",
    "             'pos_tokens': 1.0,\n",
    "#              'opinion_stats': 1.0,\n",
    "             'sentiments' : 1.0\n",
    "         },\n",
    "     )),\n",
    "    ('clf', svm.SVC()),\n",
    "], memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T04:05:27.630878Z",
     "start_time": "2017-12-22T04:03:51.528357Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52342192691\n"
     ]
    }
   ],
   "source": [
    "opinions = raw_data_pd[\"sentence\"].tolist()\n",
    "labels = raw_data_pd[\"class\"].tolist()\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 5)\n",
    "accuracy = []\n",
    "models = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(opinions, labels):\n",
    "    X_train, X_test = [opinions[i]\n",
    "                       for i in train_idx], [opinions[i] for i in test_idx]\n",
    "    y_train, y_test = [labels[i]\n",
    "                       for i in train_idx], [labels[i] for i in test_idx]\n",
    "    parameters = {\n",
    "#         'union__word_tokens__tfidf__ngram_range': ((2,2),),\n",
    "#         'union__word_tokens__tfidf__max_df' : (1.0,),\n",
    "#         'union__word_tokens__tfidf__min_df' : (0.0,),\n",
    "#         'union__word_tokens__tfidf__use_idf': (True,),\n",
    "#         'union__word_tokens__tfidf__sublinear_tf': (True,),\n",
    "#         'union__word_tokens__best__k': (5, 10),\n",
    "        'union__pos_tokens__tfidf__ngram_range': ((1,3),),\n",
    "        'union__pos_tokens__tfidf__max_df' : (1.0,),\n",
    "        'union__pos_tokens__tfidf__min_df' : (0.0,),\n",
    "        'union__pos_tokens__tfidf__use_idf': (True,),\n",
    "        'union__pos_tokens__tfidf__sublinear_tf': (True,)\n",
    "    }\n",
    "    gs_classifier = GridSearchCV(pipeline, parameters, n_jobs=-1, scoring = 'accuracy')\n",
    "    gs_classifier = gs_classifier.fit(X_train, y_train)\n",
    "    prediction = gs_classifier.predict(X_test)\n",
    "    models.append(gs_classifier)\n",
    "    accuracy.append(np.sum(y_test == prediction) * 1.0 / len(y_test))\n",
    "print np.mean(accuracy)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T04:00:45.239635Z",
     "start_time": "2017-12-22T04:00:45.230016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.58139534883720934,\n",
       "  0.44186046511627908,\n",
       "  0.53488372093023251,\n",
       "  0.52325581395348841,\n",
       "  0.5357142857142857],\n",
       " 0.52342192691029898)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T04:03:26.600495Z",
     "start_time": "2017-12-22T04:03:26.582735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.479532163743 0.581395348837\n",
      "union__pos_tokens__tfidf__max_df: 1.0\n",
      "union__pos_tokens__tfidf__min_df: 0.0\n",
      "union__pos_tokens__tfidf__ngram_range: (1, 2)\n",
      "union__pos_tokens__tfidf__sublinear_tf: True\n",
      "union__pos_tokens__tfidf__use_idf: True\n",
      "0.520467836257 0.441860465116\n",
      "union__pos_tokens__tfidf__max_df: 1.0\n",
      "union__pos_tokens__tfidf__min_df: 0.0\n",
      "union__pos_tokens__tfidf__ngram_range: (1, 2)\n",
      "union__pos_tokens__tfidf__sublinear_tf: True\n",
      "union__pos_tokens__tfidf__use_idf: True\n",
      "0.570175438596 0.53488372093\n",
      "union__pos_tokens__tfidf__max_df: 1.0\n",
      "union__pos_tokens__tfidf__min_df: 0.0\n",
      "union__pos_tokens__tfidf__ngram_range: (1, 2)\n",
      "union__pos_tokens__tfidf__sublinear_tf: True\n",
      "union__pos_tokens__tfidf__use_idf: True\n",
      "0.53216374269 0.523255813953\n",
      "union__pos_tokens__tfidf__max_df: 1.0\n",
      "union__pos_tokens__tfidf__min_df: 0.0\n",
      "union__pos_tokens__tfidf__ngram_range: (1, 2)\n",
      "union__pos_tokens__tfidf__sublinear_tf: True\n",
      "union__pos_tokens__tfidf__use_idf: True\n",
      "0.517441860465 0.535714285714\n",
      "union__pos_tokens__tfidf__max_df: 1.0\n",
      "union__pos_tokens__tfidf__min_df: 0.0\n",
      "union__pos_tokens__tfidf__ngram_range: (1, 2)\n",
      "union__pos_tokens__tfidf__sublinear_tf: True\n",
      "union__pos_tokens__tfidf__use_idf: True\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "max_df = []\n",
    "min_df = []\n",
    "for i, model in enumerate(models):\n",
    "    print model.best_score_, accuracy[i]\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print \"%s: %r\" % (param_name, model.best_params_[param_name])\n",
    "print np.mean(max_df)\n",
    "print np.mean(min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notify_time": "0",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
